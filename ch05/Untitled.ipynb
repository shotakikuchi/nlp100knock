{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CaboCha.Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('neko.txt') as rf, open('neko.txt.cabocha', mode='w') as wf:\n",
    "    for line in rf:\n",
    "        wf.write(c.parse(line).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 0/0 0.000000\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
      "EOS\n",
      "EOS\n",
      "* 0 2D 0/0 -0.764522\n",
      "　\t記号,空白,*,*,*,*,　,　,　\n",
      "* 1 2D 0/1 -0.764522\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 2 -1D 0/2 0.000000\n",
      "猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "* 0 2D 0/1 -1.911675\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 1 2D 0/0 -1.911675\n",
      "まだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -20 neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．\n",
    "このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．\n",
    "さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morph(data):\n",
    "    head, body = data.split('\\t')\n",
    "    surface = head\n",
    "    body_sp = body.split(',')\n",
    "    base = body_sp[6]\n",
    "    pos = body_sp[0]\n",
    "    pos1 = body_sp[1]\n",
    "    return Morph(surface, base, pos, pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "tmp_ret = []\n",
    "with open('neko.txt.cabocha') as rf:\n",
    "    for line in rf:\n",
    "        if 'EOS' in line:\n",
    "            ret.append(tmp_ret)\n",
    "            tmp_ret = []\n",
    "        if not ',' in line:\n",
    "            continue\n",
    "        morph = get_morph(line)\n",
    "        tmp_ret.append(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surface': '\\u3000', 'base': '\\u3000', 'pos': '記号', 'pos1': '空白'}\n",
      "{'surface': '吾輩', 'base': '吾輩', 'pos': '名詞', 'pos1': '代名詞'}\n",
      "{'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}\n",
      "{'surface': '猫', 'base': '猫', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': 'で', 'base': 'だ', 'pos': '助動詞', 'pos1': '*'}\n",
      "{'surface': 'ある', 'base': 'ある', 'pos': '助動詞', 'pos1': '*'}\n",
      "{'surface': '。', 'base': '。', 'pos': '記号', 'pos1': '句点'}\n"
     ]
    }
   ],
   "source": [
    "for r in ret[2]:\n",
    "    print(vars(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．  \n",
    "このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．  \n",
    "さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．\n",
    "第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def trim_text(self):\n",
    "        l = [m.surface for m in self.morphs if m.pos != '記号']\n",
    "        return ''.join(l)\n",
    "    \n",
    "    def is_pos(self, pos):\n",
    "        return any(m.pos == pos for m in self.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphs = []\n",
    "chunks = []\n",
    "ret = []\n",
    "\n",
    "with open('neko.txt.cabocha') as rf:\n",
    "    for line in rf:\n",
    "        if line[0] == '*':\n",
    "            if morphs:\n",
    "                chunk = Chunk(morphs, dst, [])\n",
    "                chunks.append(chunk)\n",
    "                morphs = []\n",
    "            dst = int(line.split(' ')[2][:-1])\n",
    "        \n",
    "        if ',' in line:\n",
    "            morph = get_morph(line)\n",
    "            morphs.append(morph)\n",
    "            \n",
    "        if 'EOS' in line:\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst, []))\n",
    "                for i, c in enumerate(chunks):\n",
    "                    if c.dst != -1:\n",
    "                        chunks[c.dst].srcs.append(i)\n",
    "                ret.append(chunks)\n",
    "                chunks = []\n",
    "                morphs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文節 -> 係り先\n",
      "------------------------------\n",
      "この->1\n",
      "書生というのは->7\n",
      "時々->4\n",
      "我々を->4\n",
      "捕えて->5\n",
      "煮て->6\n",
      "食うという->7\n",
      "話である。->-1\n"
     ]
    }
   ],
   "source": [
    "print('文節 -> 係り先')\n",
    "print('-'*30)\n",
    "for c in ret[7]:\n",
    "    dst = vars(c)['dst']\n",
    "    v = \"\".join([vars(m)['surface'] for m in c.morphs])\n",
    "    print(f'{v}->{dst}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．\n",
    "ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "係り元の文節\t係り先の文節\n",
      "----------------------------------------\n",
      "　\t猫である\n",
      "吾輩は\t猫である\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "　どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n"
     ]
    }
   ],
   "source": [
    "def sanitize_text(text):\n",
    "    return re.sub('[、。]', '', text)\n",
    "\n",
    "print('係り元の文節\\t係り先の文節')\n",
    "print('-'*40)\n",
    "                  \n",
    "for chunks in ret[:5]:\n",
    "    for c in chunks:\n",
    "        dst = vars(c)['dst']\n",
    "        if dst == -1:\n",
    "            continue\n",
    "        src_sentence = sanitize_text(\"\".join([vars(m)['surface'] for m in c.morphs]))\n",
    "        dst_sentence = sanitize_text(\"\".join([vars(m)['surface'] for m in chunks[dst].morphs]))\n",
    "        print(f'{src_sentence}\\t{dst_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，\n",
    "これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "係り元の文節\t係り先の文節\n",
      "----------------------------------------\n",
      "　どこで\t生れたか\n",
      "見当が\tつかぬ\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "あとで\t聞くと\n",
      "我々を\t捕えて\n",
      "掌に\t載せられて\n",
      "スーと\t持ち上げられた\n",
      "時\tフワフワした\n",
      "感じが\tあったばかりである\n"
     ]
    }
   ],
   "source": [
    "print('係り元の文節\\t係り先の文節')\n",
    "print('-'*40)\n",
    "                  \n",
    "for chunks in ret[:10]:\n",
    "    for c in chunks:\n",
    "        dst = vars(c)['dst']\n",
    "        # 係り先が-1の判定\n",
    "        if dst == -1:\n",
    "            continue\n",
    "        # 名詞を含んでいるか判定\n",
    "        if not [True for m in c.morphs if vars(m)['pos'] == '名詞']:\n",
    "            continue\n",
    "        # 係り先が動詞を含んでいるか判定\n",
    "        if not [True for m in chunks[dst].morphs if vars(m)['pos'] == '動詞']:\n",
    "            continue\n",
    "        src_sentence = sanitize_text(\"\".join([vars(m)['surface'] for m in c.morphs]))\n",
    "        dst_sentence = sanitize_text(\"\".join([vars(m)['surface'] for m in chunks[dst].morphs]))\n",
    "        print(f'{src_sentence}\\t{dst_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．  \n",
    "可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．  \n",
    "また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(sentence):\n",
    "    c = CaboCha.Parser()\n",
    "    tree =  c.parse(sentence)\n",
    "    sentence = tree.toString(CaboCha.FORMAT_LATTICE)\n",
    "    morphs = []\n",
    "    chunks = []\n",
    "    for line in sentence.split('\\n')[:-2]:\n",
    "        if line[0] == '*':\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst, []))\n",
    "            morphs = []\n",
    "            data = line.split(' ')\n",
    "            dst = int(data[2][:-1])\n",
    "        else:\n",
    "            morphs.append(get_morph(line))\n",
    "            \n",
    "    chunks.append(Chunk(morphs, dst, []))\n",
    "    for i,c in enumerate(chunks):\n",
    "        d = c.dst\n",
    "        if d != -1:\n",
    "            chunks[d].srcs.append(i)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"350pt\" height=\"280pt\"\n",
       " viewBox=\"0.00 0.00 349.92 280.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 276.05)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-276.05 345.92,-276.05 345.92,4 -4,4\"/>\n",
       "<!-- 吾輩は -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>吾輩は</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"41.01\" cy=\"-174.53\" rx=\"41.02\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-173.83\" font-family=\"Times,serif\" font-size=\"14.00\">吾輩は</text>\n",
       "</g>\n",
       "<!-- 猫だから -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>猫だから</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"161.01\" cy=\"-97.52\" rx=\"50.82\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.01\" y=\"-96.82\" font-family=\"Times,serif\" font-size=\"14.00\">猫だから</text>\n",
       "</g>\n",
       "<!-- 吾輩は&#45;&gt;猫だから -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>吾輩は&#45;&gt;猫だから</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.88,-157.99C83.26,-147.12 106.72,-132.46 125.99,-120.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.89,-123.35 134.52,-115.08 124.18,-117.41 127.89,-123.35\"/>\n",
       "</g>\n",
       "<!-- ある -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ある</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"216.01\" cy=\"-20.51\" rx=\"31.23\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"216.01\" y=\"-19.81\" font-family=\"Times,serif\" font-size=\"14.00\">ある</text>\n",
       "</g>\n",
       "<!-- 猫だから&#45;&gt;ある -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>猫だから&#45;&gt;ある</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.89,-77.59C181.54,-68.53 189.6,-57.53 196.82,-47.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.8,-49.53 202.9,-39.4 194.16,-45.39 199.8,-49.53\"/>\n",
       "</g>\n",
       "<!-- 二十世紀の -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>二十世紀の</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"161.01\" cy=\"-174.53\" rx=\"60.62\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.01\" y=\"-173.83\" font-family=\"Times,serif\" font-size=\"14.00\">二十世紀の</text>\n",
       "</g>\n",
       "<!-- 二十世紀の&#45;&gt;猫だから -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>二十世紀の&#45;&gt;猫だから</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.01,-153.82C161.01,-145.98 161.01,-136.79 161.01,-128.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"164.51,-128.1 161.01,-118.1 157.51,-128.1 164.51,-128.1\"/>\n",
       "</g>\n",
       "<!-- この -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>この</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"291.01\" cy=\"-251.54\" rx=\"31.23\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"291.01\" y=\"-250.84\" font-family=\"Times,serif\" font-size=\"14.00\">この</text>\n",
       "</g>\n",
       "<!-- くらいの -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>くらいの</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"291.01\" cy=\"-174.53\" rx=\"50.82\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"291.01\" y=\"-173.83\" font-family=\"Times,serif\" font-size=\"14.00\">くらいの</text>\n",
       "</g>\n",
       "<!-- この&#45;&gt;くらいの -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>この&#45;&gt;くらいの</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M291.01,-230.83C291.01,-223 291.01,-213.81 291.01,-205.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.51,-205.11 291.01,-195.11 287.51,-205.11 294.51,-205.11\"/>\n",
       "</g>\n",
       "<!-- 教育は -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>教育は</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"281.01\" cy=\"-97.52\" rx=\"41.02\" ry=\"20.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.01\" y=\"-96.82\" font-family=\"Times,serif\" font-size=\"14.00\">教育は</text>\n",
       "</g>\n",
       "<!-- くらいの&#45;&gt;教育は -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>くらいの&#45;&gt;教育は</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.38,-153.82C287.34,-145.98 286.11,-136.79 284.97,-128.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"288.41,-127.55 283.62,-118.1 281.47,-128.47 288.41,-127.55\"/>\n",
       "</g>\n",
       "<!-- 教育は&#45;&gt;ある -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>教育は&#45;&gt;ある</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M265.28,-78.36C256.95,-68.75 246.61,-56.82 237.55,-46.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.13,-43.99 230.93,-38.72 234.84,-48.57 240.13,-43.99\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11613c978>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "sentence = \"吾輩は二十世紀の猫だからこのくらいの教育はある。\"\n",
    "chunks = chunking(sentence)\n",
    "for c in chunks:\n",
    "    if c.dst != -1:\n",
    "        dot.edge(c.trim_text() , chunks[c.dst].trim_text())\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい．  \n",
    "動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． \n",
    "ただし，出力は以下の仕様を満たすようにせよ．  \n",
    "\n",
    "動詞を含む文節において，最左の動詞の基本形を述語とする  \n",
    "述語に係る助詞を格とする  \n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる  \n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\n",
      "つく\tか が\n",
      "する\t\n",
      "泣く\tで\n",
      "する\tて だけ は\n",
      "始める\tで\n",
      "見る\tは を\n",
      "聞く\tで\n",
      "捕える\tを\n",
      "煮る\tて\n"
     ]
    }
   ],
   "source": [
    "pattern = []\n",
    "for chunks in ret:\n",
    "    found = [c for c in chunks if c.is_pos('動詞')]\n",
    "    for c in found:\n",
    "        src_chunks = [chunks[i] for i in c.srcs]\n",
    "        pos = [m.base for c in src_chunks for m in c.morphs if m.pos == '助詞']\n",
    "        left_verb = next(m.base for m in c.morphs if m.pos == '動詞')\n",
    "        pattern.append('\\t'.join((left_verb, ' '.join(pos))))\n",
    "print(*pattern[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pattern.txt','w') as f:\n",
    "    t = '\\n'.join(pattern)\n",
    "    f.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 565 云う\tと\n",
      " 442 する\tを\n",
      " 435 する\t\n",
      " 249 思う\tと\n",
      " 199 ある\tが\n",
      " 189 なる\tに\n",
      " 174 する\tに\n",
      " 173 見る\tて\n",
      " 127 する\tと\n",
      " 122 云う\t\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sort pattern.txt | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 442 する\tを\n",
      " 435 する\t\n",
      " 174 する\tに\n",
      " 127 する\tと\n",
      " 117 する\tが\n",
      "  84 する\tて を\n",
      "  59 する\tは\n",
      "  58 する\tを に\n",
      "  58 する\tて\n",
      "  51 する\tが を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep \"^する\" pattern.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 173 見る\tて\n",
      "  94 見る\tを\n",
      "  80 見る\t\n",
      "  21 見る\tて て\n",
      "  20 見る\tから\n",
      "  16 見る\tて を\n",
      "  14 見る\tと\n",
      "  12 見る\tで\n",
      "  11 見る\tから て\n",
      "  11 見る\tは て\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep \"^見る\" pattern.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 与える\tに を\n",
      "   1 与える\tけれども に は を\n",
      "   1 与える\tじゃあ か と は て を\n",
      "   1 与える\tとして を か\n",
      "   1 与える\tたり て に を\n",
      "   1 与える\tで だけ に を\n",
      "   1 与える\tに は に対して のみ は も\n",
      "   1 与える\tて が は は と て に を\n",
      "   1 与える\tは て に を に\n",
      "   1 与える\tは て に を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep \"^与える\" pattern.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．  \n",
    "45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）  \n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる  \n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\t　 どこ で\n",
      "つく\tか が\t生れ た か とんと 見当 が\n",
      "する\t\t\n",
      "泣く\tで\t所 で ニャーニャー\n",
      "する\tて だけ は\t泣い て いた事 だけ は\n",
      "始める\tで\tここ で\n",
      "見る\tは を\t吾輩 は もの を\n",
      "聞く\tで\tあと で\n",
      "捕える\tを\t時々 我々 を\n",
      "煮る\tて\t捕え て\n"
     ]
    }
   ],
   "source": [
    "pattern = []\n",
    "for chunks in ret:\n",
    "    found = [c for c in chunks if c.is_pos('動詞')]\n",
    "    for c in found:\n",
    "        src_chunks = [chunks[i] for i in c.srcs]\n",
    "        pos = [m.base for c in src_chunks for m in c.morphs if m.pos == '助詞']\n",
    "        term = [m.surface for c in src_chunks for m in c.morphs]\n",
    "        left_verb = next(m.base for m in c.morphs if m.pos == '動詞')\n",
    "        pattern.append('\\t'.join((left_verb, ' '.join(pos), ' '.join(term))))\n",
    "print(*pattern[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．  \n",
    "\n",
    "「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする  \n",
    "\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる  \n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる  \n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）  \n",
    "- 例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．  \n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決心をする\tと\tこう と\n",
      "返報をする\tんで\t偸 んで\n",
      "昼寝をする\t\tよく\n",
      "昼寝をする\tが\t彼 が\n",
      "迫害を加える\tて\tしかも しよ う もの なら 追い廻し て\n",
      "話をする\t\t\n",
      "投書をする\tて へ\tやっ て ほととぎす へ\n",
      "話をする\tに\t時 に\n",
      "写生をする\t\t思う なら ちと\n",
      "昼寝をする\tて\t出 て 善く\n"
     ]
    }
   ],
   "source": [
    "pattern = []\n",
    "for chunks in ret:\n",
    "    found = [c for c in chunks if c.is_pos('動詞')]\n",
    "    for c in found:\n",
    "        src_chunks = [chunks[i] for i in c.srcs]\n",
    "        for sc in src_chunks:\n",
    "            mf = sc.morphs\n",
    "            if len(mf) > 1 and mf[0].pos1 == 'サ変接続' and mf[1].base == 'を':\n",
    "                pos = [m.base for c in src_chunks for m in c.morphs if m.pos == '助詞'][:-1]\n",
    "                term = [sanitize_text(m.surface) for c in src_chunks for m in c.morphs][:-2]\n",
    "                left_verb = next(mf[0].base + 'を' + m.base for m in c.morphs if m.pos == '動詞')\n",
    "                pattern.append('\\t'.join((left_verb, ' '.join(pos), ' '.join(term))))\n",
    "print(*pattern[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sahenwo.txt','w') as f:\n",
    "    t = '\\n'.join(pattern)\n",
    "    f.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29 返事をする\n",
      "  21 挨拶をする\n",
      "  14 真似をする\n",
      "  14 話をする\n",
      "  11 喧嘩をする\n",
      "   8 質問をする\n",
      "   7 運動をする\n",
      "   6 昼寝をする\n",
      "   6 話を聞く\n",
      "   5 質問をかける\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cut -f1 sahenwo.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ．   \n",
    "ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "for chunks in ret:\n",
    "    found = [c for c in chunks if c.is_pos('名詞')]\n",
    "    for chunk in found:\n",
    "        l = [chunk.trim_text()]\n",
    "        dst = chunk.dst\n",
    "        while True:\n",
    "            if dst == -1:\n",
    "                break\n",
    "            next_chunk = chunks[dst]\n",
    "            l.append(next_chunk.trim_text())\n",
    "            dst = next_chunk.dst\n",
    "        pattern.append('->'.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\n",
      "吾輩は->猫である\n",
      "猫である\n",
      "名前は->無い\n",
      "どこで->生れたか->つかぬ\n",
      "見当が->つかぬ\n",
      "何でも->薄暗い->所で->泣いて->記憶している\n",
      "所で->泣いて->記憶している\n",
      "ニャーニャー->泣いて->記憶している\n",
      "いた事だけは->記憶している\n"
     ]
    }
   ],
   "source": [
    "print(*pattern[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示  \n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "====================\n",
    "noun_ix: [0, 1, 3, 4]\n",
    "[0, 5]\n",
    "吾輩は\t見た\n",
    "[1, 2, 3, 4, 5]\n",
    "ここで\t始めて\t人間という\tものを\t見た\n",
    "[3, 4, 5]\n",
    "人間という\tものを\t見た\n",
    "[4, 5]\n",
    "ものを\t見た\n",
    "====================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "noun_ix = [0, 1, 3, 4]\n",
    "indices = [\n",
    "    [0, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [3, 4, 5],\n",
    "    [4, 5]\n",
    "]\n",
    "for i in indices:\n",
    "    if len(set(i) & set(noun_ix)) > 1:\n",
    "        \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(l=[1,2,3,4,5],s=[0,1,2,3], pairs_length=2):\n",
    "    ret = []\n",
    "    if len(set(l) & set(s)) < 2:\n",
    "        return ret\n",
    "    for i,n in enumerate(l):\n",
    "        if n in s:\n",
    "            for j in l[i+1:]:\n",
    "                if j in s:\n",
    "                    ret.append([n, j])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 3], [2, 3]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_ix: [1, 2]\n",
      "[1, 2]\n",
      "吾輩は\t猫である\n",
      "[2]\n",
      "猫である\n",
      "====================\n",
      "noun_ix: [0, 3]\n",
      "[0, 1, 4]\n",
      "どこで\t生れたか\tつかぬ\n",
      "[3, 4]\n",
      "見当が\tつかぬ\n",
      "====================\n",
      "noun_ix: [0, 3, 4, 6, 7]\n",
      "[0, 1, 3, 5, 7]\n",
      "何でも\t薄暗い\t所で\t泣いて\t記憶している\n",
      "[3, 5, 7]\n",
      "所で\t泣いて\t記憶している\n",
      "[4, 5, 7]\n",
      "ニャーニャー\t泣いて\t記憶している\n",
      "[6, 7]\n",
      "いた事だけは\t記憶している\n",
      "[7]\n",
      "記憶している\n",
      "====================\n",
      "noun_ix: [0, 1, 3, 4]\n",
      "[0, 5]\n",
      "吾輩は\t見た\n",
      "[1, 2, 3, 4, 5]\n",
      "ここで\t始めて\t人間という\tものを\t見た\n",
      "[3, 4, 5]\n",
      "人間という\tものを\t見た\n",
      "[4, 5]\n",
      "ものを\t見た\n",
      "====================\n",
      "noun_ix: [1, 3, 4, 5, 6, 7, 8]\n",
      "[1, 2, 8]\n",
      "あとで\t聞くと\t種族であったそうだ\n",
      "[3, 8]\n",
      "それは\t種族であったそうだ\n",
      "[4, 5, 8]\n",
      "書生という\t人間中で\t種族であったそうだ\n",
      "[5, 8]\n",
      "人間中で\t種族であったそうだ\n",
      "[6, 7, 8]\n",
      "一番\t獰悪な\t種族であったそうだ\n",
      "[7, 8]\n",
      "獰悪な\t種族であったそうだ\n",
      "[8]\n",
      "種族であったそうだ\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for chunks in ret[1:7]:\n",
    "    # 名詞句の文節のindex\n",
    "    noun_ix = [i for i,c in enumerate(chunks) if c.is_pos('名詞')]\n",
    "    # 名詞句のペアが無い場合\n",
    "    if len(noun_ix) < 2:\n",
    "        continue\n",
    "    indices = []\n",
    "    l = []\n",
    "    print(f'noun_ix: {noun_ix}')\n",
    "    for ix in noun_ix:\n",
    "        indices.append(ix)\n",
    "        next_dst = chunks[ix].dst\n",
    "        while next_dst != -1:\n",
    "            indices.append(next_dst)\n",
    "            next_dst = chunks[next_dst].dst\n",
    "#         if not len(set(noun_ix) & set(indices)) > 1:\n",
    "#             indices = []\n",
    "#             continue\n",
    "        l.append(indices)\n",
    "        print(indices)\n",
    "        print(\"\\t\".join([chunks[ix].trim_text() for ix in indices]))\n",
    "        indices = []\n",
    "    print('='*20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([0, 3, 4, 6, 7]) & set([7])) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'morphs': [<__main__.Morph at 0x10a522390>, <__main__.Morph at 0x10a522320>],\n",
       " 'dst': 2,\n",
       " 'srcs': []}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(found[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'morphs': [<__main__.Morph at 0x10a522400>,\n",
       "  <__main__.Morph at 0x10a5224a8>,\n",
       "  <__main__.Morph at 0x10a5223c8>,\n",
       "  <__main__.Morph at 0x10a522438>],\n",
       " 'dst': -1,\n",
       " 'srcs': [0, 1]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "for chunks in ret:\n",
    "    found = [c for c in chunks if c.is_pos('名詞')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
